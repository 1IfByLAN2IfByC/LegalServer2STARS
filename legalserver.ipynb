{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import json\n",
    "import xmltodict as xml\n",
    "import logging\n",
    "import pdb\n",
    "import logging.config\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path='/users/mikelee/git/TLSC/configs/logconfig.json'\n",
    "default_level=logging.INFO\n",
    "env_key='LOG_CFG'\n",
    "\n",
    "\n",
    "with open(default_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "logging.config.dictConfig(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api settings\n",
    "username = 'mlee'\n",
    "pw = 'Michael91'\n",
    "apiget = 'https://tlsc.legalserver.org/modules/report/api_export.php?load=354&api_key=5c267c1e-c3d8-44be-a8fd-e3fcac1e021a'\n",
    "# apiget = 'https://tlsc.legalserver.org//modules/report/api_export.php?load=359&api_key=5c27be11-db50-4285-abc1-dd71ac1e021a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    " \n",
    "# reconfigure the data to a nice format\n",
    "# datareturn = json.dumps(xml.parse(datareturn.text))\n",
    "# datareturn = json.loads(datareturn)\n",
    "# datareturn = xml.parse(datareturn.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSClient(object):\n",
    "    def __init__(self, apikey, auth, permstore, log):\n",
    "        '''\n",
    "        inputs:\n",
    "        -------\n",
    "            apikey: string\n",
    "                url corresponding to the report XML API\n",
    "            \n",
    "            auth: tuple of strings\n",
    "                (username, password) for http authentication\n",
    "                \n",
    "            permstore: string\n",
    "                filelocation where the uploaded ids can be stored \n",
    "                persistently in case of shutdown\n",
    "            \n",
    "            log: logging log\n",
    "                log to store runs and id updates\n",
    "            \n",
    "        parameters:\n",
    "        -----------\n",
    "            reportids: list of ints\n",
    "                all the report IDs that have already been uploaded\n",
    "                \n",
    "            toupload: list of dicts\n",
    "                row object returned by requests that are not in reportids\n",
    "                (i.e. the need to be uploaded)      \n",
    "        '''\n",
    "        \n",
    "        self.apikey = apikey\n",
    "        self.auth = auth\n",
    "        self.permstore = permstore\n",
    "        self.log = log\n",
    "    \n",
    "        self.reportids = []\n",
    "        self.toupload = []\n",
    "    \n",
    "    def _id_check(self):\n",
    "        '''open the file where the permstore is and check if it is empty'''\n",
    "        \n",
    "        try:\n",
    "            with open(self.permstore, 'r') as f:\n",
    "                tmp = f.readlines()\n",
    "\n",
    "            self.reportids = [int(line) for line in tmp]\n",
    "        except OSError:\n",
    "            self.log.debug('File Not Found Error. Creating file at {}'.format(self.permstore))\n",
    "            \n",
    "            with open(self.permstore, 'w+') as f:\n",
    "                pass\n",
    " \n",
    "    def _pull_report(self):\n",
    "        '''\n",
    "        pull the report using requesets. authentication handled using http auth\n",
    "        '''\n",
    "        # pull\n",
    "        self.log.debug('Attempting to make request')\n",
    "        datareturn = rq.get(self.apikey, auth=(self.auth[0], self.auth[1]))\n",
    "        \n",
    "        # check datareturn error codes\n",
    "        if datareturn.status_code != 200:\n",
    "            self.log.debug('Bad request. Error code: {}'.format(datareturn.status_code))\n",
    "            # raise some error\n",
    "            raise ValueError\n",
    "            \n",
    "        else:\n",
    "            self.log.debug('Request made successfully')\n",
    "            # reconfigure the data to a nice format\n",
    "            datareturn = json.dumps(xml.parse(datareturn.text))\n",
    "            datareturn = json.loads(datareturn)\n",
    "            if isinstance(datareturn['report']['row'], dict):\n",
    "                # only a single record was found, transfrom to list\n",
    "                datareturn['report']['row'] = [datareturn['report']['row']]\n",
    "            for row in datareturn['report']['row']:\n",
    "                if int(row['id']) not in self.reportids:\n",
    "                    # append to list of completed ids\n",
    "                    self.reportids.append(int(row['id']))\n",
    "                    self.toupload.append(row)\n",
    "                    self.log.info('New case: {}'.format(row['id']))\n",
    "            \n",
    "            self.log.info(\n",
    "                '\\n --------------------------------------------- \\n \\\n",
    "                Found {n} new values to be uploaded \\n \\\n",
    "                ---------------------------------------------'.format(n=len(self.toupload)))\n",
    "            \n",
    "            with open(self.permstore, 'a') as f:\n",
    "                for line in self.reportids:\n",
    "                    f.write(str(line))\n",
    "            \n",
    "            self.log.debug('Permanet storage updated')\n",
    "        \n",
    "\n",
    "class TLSCRequestScrubber(object):\n",
    "    def __init__(self, keymapping, errorfile, log):\n",
    "        self.keyfile = keymapping\n",
    "        self.log = log\n",
    "        self.errorfile = errorfile\n",
    "        \n",
    "        self._get_mapping()\n",
    "        \n",
    "    def _get_mapping(self):\n",
    "        try:\n",
    "            with open(self.keyfile, 'r') as f:\n",
    "                self.keymap = json.loads(f)\n",
    "        except OSError:\n",
    "            self.log.error('No key file found at: {}'.format(self.keyfile))\n",
    "        \n",
    "    def request_mapper(self, request):\n",
    "        '''\n",
    "        map the request objects onto the key values given by the map file\n",
    "        \n",
    "        inputs:\n",
    "        -------\n",
    "            request: list of dicts\n",
    "                list of request objects\n",
    "        '''\n",
    "        \n",
    "        reformatted = []\n",
    "        errors = []\n",
    "        \n",
    "        for row in reqeuest:\n",
    "            stardata = {}\n",
    "            for k in row.keys():\n",
    "                try:\n",
    "                    stardata[self.keymap[row[k]]['starkey']] =  self.keymap[row[k]['starvalue']]\n",
    "                    reformatted.append(stardata)\n",
    "                except KeyError:\n",
    "                    self.log.error('Key pulled from TLSC database not found. Key: {}'.format(\n",
    "                        k))\n",
    "                    errors.append(row)\n",
    "                    \n",
    "        if len(errors) != 0:            \n",
    "            with open(self.errorfile, 'a') as f:\n",
    "                json.dump(errors, f)\n",
    "            \n",
    "        self.log.info('{} matters reformatted'.format(len(reformatted)))\n",
    "        self.log.info('{n} matters had errors. View them here: {e}'.format(\n",
    "            n=len(errors), e=self.errorfile))\n",
    "        \n",
    "        return reformatted\n",
    "\n",
    "\n",
    "class STARSException(Exception):\n",
    "    pass\n",
    "\n",
    "class STARSClient(object):\n",
    "    '''\n",
    "    Client for connecting to the STAR database and uploading scrubbed Legal Server \n",
    "    reports.\n",
    "    \n",
    "    inputs:\n",
    "    -------\n",
    "        apikey: str\n",
    "            url of the STARS api \n",
    "        \n",
    "        auth: tuple of strings\n",
    "            (username, password) for STARS\n",
    "        \n",
    "        startechspec: string\n",
    "            file location of the STAR tech spec YAML. Used\n",
    "            to typecheck and validate input\n",
    "        \n",
    "        log: logging handler\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, apikey, auth, startechspec, log):\n",
    "        self.apikey = apikey\n",
    "        self.auth = auth\n",
    "        self.startechspec = startechspec\n",
    "        self.log = log\n",
    "    \n",
    "        self._load_tech_spec()\n",
    "        \n",
    "    def _typemapping(self):\n",
    "        '''generate type map. removed for claritys sake '''\n",
    "        self.typemap = {\n",
    "            'integer': int,\n",
    "            'boolean': bool,\n",
    "            'array': list,\n",
    "            'object': dict,\n",
    "            'string': str\n",
    "        }\n",
    "        \n",
    "    def _load_tech_spec(self):\n",
    "        try:\n",
    "            with open(self.startechspec, 'r') as f:\n",
    "                self.startechspec = yaml.load(f)\n",
    "        except OSError:\n",
    "            self.log.error('No tech spec file found. Address given: {}'.format(\n",
    "                self.startechspec))\n",
    "\n",
    "        \n",
    "    def _validate_input(self, lsdata):\n",
    "        '''\n",
    "        validate the type of each row against the STAR tech spec\n",
    "        \n",
    "        inputs:\n",
    "        -------\n",
    "            lsdata: dict\n",
    "                individual row in the report to be validated\n",
    "        '''\n",
    "\n",
    "        for key, value in lsdata.itmes():   \n",
    "            try:\n",
    "                validate = self.startechspec['definitions']['SHIP_Beneficiary_Data']['properties'][key]['type'] \n",
    "                if validate != value:\n",
    "                    self.log.error('Reformatted data has does not match TYPE in tech spec. Value: {}'.format(value))\n",
    "                    raise STARSException('Reformatted data has does not match TYPE in tech spec. Value: {}'.format(value))\n",
    "\n",
    "            except KeyError:\n",
    "                self.log.error('Reformatted data has key not in tech spec. Key: {}'.format(key))\n",
    "                raise STARSException('Reformatted data has key not in tech spec. Key: {}'.format(key))\n",
    "\n",
    "    def upload_to_STARS(self, lsdata):\n",
    "        '''\n",
    "        do we need to upload as a list or as indivual items? \n",
    "        \n",
    "        inputs:\n",
    "        -------\n",
    "            lsdata: list of dicts\n",
    "                scrubbed Legal Server data of new cases to upload\n",
    "        '''\n",
    "        \n",
    "        for row in lsdata:\n",
    "            try:\n",
    "                self._validate_input(row)\n",
    "            except STARSException as e:\n",
    "                self.log.error('Legal Server data is invalid', e)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsserver = LSClient(apiget, (username, pw), '/users/mikelee/git/TLSC/testfile.txt', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-29 15:48:14,778 - __main__ - INFO - New case: 351450\n",
      "2018-12-29 15:48:14,780 - __main__ - INFO - \n",
      " --------------------------------------------- \n",
      "                 Found 1 new values to be uploaded \n",
      "                 ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lsserver._pull_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alamosa'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsserver.toupload[0]['address_builtin_lookup_county_county_expn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.json', 'w') as f:\n",
    "    json.dump(lsserver.toupload, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./STARS_TechSpecs_031918.yaml', 'r') as f:\n",
    "    x = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'integer'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['definitions']['SHIP_Beneficiary_Data']['properties']['beneficiaryAgeGroup']['type'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typemap[x['definitions']['SHIP_Beneficiary_Data']['properties']['beneficiaryAgeGroup']['type'] ] == type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 351450\n",
      "intake_date 2018-09-10T00:00:00-00:00\n",
      "date_application None\n",
      "phone_other None\n",
      "identification_number 18-0351450\n",
      "caseworker_name Deutsch, Melissa\n",
      "age_at_intake None\n",
      "lsc_compliant_assets None\n",
      "email None\n",
      "first Delete\n",
      "person_builtin_lookup_gender_gender_expn None\n",
      "last Delete\n",
      "initial_gross_monthly_income None\n",
      "adjusted_income_monthly 0.00\n",
      "phone_home ['999-999-9999', '999-999-9999']\n",
      "phone_business None\n",
      "phone_mobile None\n",
      "person_builtin_lookup_race_race_expn None\n",
      "address_builtin_lookup_county_county_expn Alamosa\n",
      "person_builtin_lookup_language_language_expn English\n",
      "custom_custom_matter_how_did_beneficiary_learn_about_ship__110_custom_lookup_4ebb5d2ffcacbab3e1a44ab6139efdff_expn Not Collected\n",
      "matter_builtin_lookup_problem_code_legal_problem_code_expn None\n",
      "lsc_compliant_special_problem_code None\n",
      "custom_custom_matter_medicaid_101_custom_lookup_23ae6ed8fbabe97de2f96e412f81607f_expn Claims/Billing\n",
      "custom_custom_matter_medicare_advantage__ma___mapd__102_custom_lookup_5c990489bc8258c0c9b6bc842701dac9_expn Appeals/Grievances\n",
      "custom_custom_matter_medicare_part_d_103_custom_lookup_31ca43dbf02c6b004d6067f5680778f6_expn Disenrollment\n",
      "custom_custom_matter_medigap___medicare_select_104_custom_lookup_45d881eb151a9588c0196eb4744bf220_expn Fraud and Abuse\n",
      "matter_builtin_lookup_intake_types_intake_type_expn Quick Intake\n",
      "custom_custom_matter_original_medicare_parts_a___b_105_custom_lookup_540065c45a9ec44bc144f1dcec5afb7f_expn Claims/Billing, Fraud and Abuse\n",
      "custom_custom_matter_other_insurance_106_custom_lookup_002ab22a3b76aca6f0dc7cd370be85ac_expn Indian Health Services, Long Term Care (LTC) Insurance, LTC Partnership\n",
      "custom_custom_matter_other_prescription_assistance_107_custom_lookup_5aa32232d9e865a539fa4c2b60d48dfd_expn Military Drug Benefits, Other, State Pharmaceutical Assistance Programs\n",
      "custom_custom_matter_part_d_low_income_subsidy_lis___extra_help_108_custom_lookup_86a9df4d7c0c72b73fde43941a2b172d_expn Appeals/Grievances, Eligibility/Screening\n",
      "receiving_or_applying_for_social_security_disability_or_med_112 t\n",
      "perm_zip None\n",
      "matter_builtin_lookup_case_disposition_case_disposition_expn Open\n",
      "duration None\n"
     ]
    }
   ],
   "source": [
    "for k,v in lsserver.toupload[0].items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "typemap = {\n",
    "            'integer': int\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
